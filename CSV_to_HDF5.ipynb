{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SSCS dataset conversion to HDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import zipfile\n",
    "import h5py\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-31 21:58:00,589\tINFO worker.py:1625 -- Started a local Ray instance.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "    <div style=\"margin-left: 50px;display: flex;flex-direction: row;align-items: center\">\n",
       "        <h3 style=\"color: var(--jp-ui-font-color0)\">Ray</h3>\n",
       "        <svg version=\"1.1\" id=\"ray\" width=\"3em\" viewBox=\"0 0 144.5 144.6\" style=\"margin-left: 3em;margin-right: 3em\">\n",
       "            <g id=\"layer-1\">\n",
       "                <path fill=\"#00a2e9\" class=\"st0\" d=\"M97.3,77.2c-3.8-1.1-6.2,0.9-8.3,5.1c-3.5,6.8-9.9,9.9-17.4,9.6S58,88.1,54.8,81.2c-1.4-3-3-4-6.3-4.1\n",
       "                    c-5.6-0.1-9.9,0.1-13.1,6.4c-3.8,7.6-13.6,10.2-21.8,7.6C5.2,88.4-0.4,80.5,0,71.7c0.1-8.4,5.7-15.8,13.8-18.2\n",
       "                    c8.4-2.6,17.5,0.7,22.3,8c1.3,1.9,1.3,5.2,3.6,5.6c3.9,0.6,8,0.2,12,0.2c1.8,0,1.9-1.6,2.4-2.8c3.5-7.8,9.7-11.8,18-11.9\n",
       "                    c8.2-0.1,14.4,3.9,17.8,11.4c1.3,2.8,2.9,3.6,5.7,3.3c1-0.1,2,0.1,3,0c2.8-0.5,6.4,1.7,8.1-2.7s-2.3-5.5-4.1-7.5\n",
       "                    c-5.1-5.7-10.9-10.8-16.1-16.3C84,38,81.9,37.1,78,38.3C66.7,42,56.2,35.7,53,24.1C50.3,14,57.3,2.8,67.7,0.5\n",
       "                    C78.4-2,89,4.7,91.5,15.3c0.1,0.3,0.1,0.5,0.2,0.8c0.7,3.4,0.7,6.9-0.8,9.8c-1.7,3.2-0.8,5,1.5,7.2c6.7,6.5,13.3,13,19.8,19.7\n",
       "                    c1.8,1.8,3,2.1,5.5,1.2c9.1-3.4,17.9-0.6,23.4,7c4.8,6.9,4.6,16.1-0.4,22.9c-5.4,7.2-14.2,9.9-23.1,6.5c-2.3-0.9-3.5-0.6-5.1,1.1\n",
       "                    c-6.7,6.9-13.6,13.7-20.5,20.4c-1.8,1.8-2.5,3.2-1.4,5.9c3.5,8.7,0.3,18.6-7.7,23.6c-7.9,5-18.2,3.8-24.8-2.9\n",
       "                    c-6.4-6.4-7.4-16.2-2.5-24.3c4.9-7.8,14.5-11,23.1-7.8c3,1.1,4.7,0.5,6.9-1.7C91.7,98.4,98,92.3,104.2,86c1.6-1.6,4.1-2.7,2.6-6.2\n",
       "                    c-1.4-3.3-3.8-2.5-6.2-2.6C99.8,77.2,98.9,77.2,97.3,77.2z M72.1,29.7c5.5,0.1,9.9-4.3,10-9.8c0-0.1,0-0.2,0-0.3\n",
       "                    C81.8,14,77,9.8,71.5,10.2c-5,0.3-9,4.2-9.3,9.2c-0.2,5.5,4,10.1,9.5,10.3C71.8,29.7,72,29.7,72.1,29.7z M72.3,62.3\n",
       "                    c-5.4-0.1-9.9,4.2-10.1,9.7c0,0.2,0,0.3,0,0.5c0.2,5.4,4.5,9.7,9.9,10c5.1,0.1,9.9-4.7,10.1-9.8c0.2-5.5-4-10-9.5-10.3\n",
       "                    C72.6,62.3,72.4,62.3,72.3,62.3z M115,72.5c0.1,5.4,4.5,9.7,9.8,9.9c5.6-0.2,10-4.8,10-10.4c-0.2-5.4-4.6-9.7-10-9.7\n",
       "                    c-5.3-0.1-9.8,4.2-9.9,9.5C115,72.1,115,72.3,115,72.5z M19.5,62.3c-5.4,0.1-9.8,4.4-10,9.8c-0.1,5.1,5.2,10.4,10.2,10.3\n",
       "                    c5.6-0.2,10-4.9,9.8-10.5c-0.1-5.4-4.5-9.7-9.9-9.6C19.6,62.3,19.5,62.3,19.5,62.3z M71.8,134.6c5.9,0.2,10.3-3.9,10.4-9.6\n",
       "                    c0.5-5.5-3.6-10.4-9.1-10.8c-5.5-0.5-10.4,3.6-10.8,9.1c0,0.5,0,0.9,0,1.4c-0.2,5.3,4,9.8,9.3,10\n",
       "                    C71.6,134.6,71.7,134.6,71.8,134.6z\"/>\n",
       "            </g>\n",
       "        </svg>\n",
       "        <table>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Python version:</b></td>\n",
       "                <td style=\"text-align: left\"><b>3.10.11</b></td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Ray version:</b></td>\n",
       "                <td style=\"text-align: left\"><b> 2.4.0</b></td>\n",
       "            </tr>\n",
       "            \n",
       "        </table>\n",
       "    </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "RayContext(dashboard_url='', python_version='3.10.11', ray_version='2.4.0', ray_commit='4479f66d4db967d3c9dd0af2572061276ba926ba', address_info={'node_ip_address': '127.0.0.1', 'raylet_ip_address': '127.0.0.1', 'redis_address': None, 'object_store_address': 'tcp://127.0.0.1:62176', 'raylet_socket_name': 'tcp://127.0.0.1:62368', 'webui_url': '', 'session_dir': 'C:\\\\Users\\\\andre\\\\AppData\\\\Local\\\\Temp\\\\ray\\\\session_2023-05-31_21-57-58_489556_11368', 'metrics_export_port': 63080, 'gcs_address': '127.0.0.1:63539', 'address': '127.0.0.1:63539', 'dashboard_agent_listen_port': 52365, 'node_id': 'caafb755d038d64162bd29c3877cdb028a6145401a05b52ccd88b1a4'})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import psutil\n",
    "import ray\n",
    "\n",
    "num_cpus = psutil.cpu_count(logical=False)\n",
    "ray.init(num_cpus=num_cpus, num_gpus=0, ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipname = \"Datasets/SynthSalienceChoralSet_v1.zip\"\n",
    "h5_pathname = \"Datasets/HDF5/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_conformity(name):\n",
    "    return re.sub(\"[~\\\"#%&*:<>?/\\\\{|}]\", \"\", name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sscs_get_split(split='train'):\n",
    "    splitname = \"Datasets/SynthSalienceChoralSet_dataSplits.json\"\n",
    "    if(split.lower() == 'train' or split.lower() == 'validate' or\n",
    "       split.lower() == 'test'):\n",
    "        return json.load(open(splitname, 'r'))[split.lower()]\n",
    "    else:\n",
    "        raise NameError(\"Split should be 'train', 'validate' or 'test'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkIntegrity(songlist):\n",
    "\n",
    "    count = 0\n",
    "    filtered_list = songlist\n",
    "    \n",
    "    with zipfile.ZipFile(zipname, \"r\") as zf:\n",
    "        ziplist = zf.namelist()\n",
    "    \n",
    "    for song in songlist:\n",
    "        fname = \"sscs/\" + song\n",
    "        mix = fname + \"_mix.csv\"\n",
    "        s = fname + \"_S.csv\"\n",
    "        a = fname + \"_A.csv\"\n",
    "        t = fname + \"_T.csv\"\n",
    "        b = fname + \"_B.csv\"\n",
    "        if  (not mix in ziplist) or \\\n",
    "            (not s in ziplist) or \\\n",
    "            (not a in ziplist) or \\\n",
    "            (not t in ziplist) or \\\n",
    "            (not b in ziplist):\n",
    "                filtered_list.remove(song)\n",
    "                with open(h5_pathname + \"Problematic_Songs.txt\", \"a\") as f:\n",
    "                    f.write(song + \"\\n\")\n",
    "                count += 1\n",
    "\n",
    "    print(f\"{count} songs not present and removed from scanlist.\")\n",
    "    return filtered_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 songs not present and removed from scanlist.\n",
      "2 songs not present and removed from scanlist.\n",
      "3 songs not present and removed from scanlist.\n"
     ]
    }
   ],
   "source": [
    "train = checkIntegrity(sscs_get_split())\n",
    "validate = checkIntegrity(sscs_get_split('validate'))\n",
    "test = checkIntegrity(sscs_get_split('test'))\n",
    "\n",
    "train_conformity = [name_conformity(name) for name in train]\n",
    "validate_conformity = [name_conformity(name) for name in validate]\n",
    "test_conformity = [name_conformity(name) for name in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_MEM = 2 * 1024 * 1024 * 1024\n",
    "\n",
    "def csv_to_df(songname):\n",
    "    with zipfile.ZipFile(zipname, mode='r') as zf:\n",
    "        fnames = [\"sscs/\" + songname + \"_mix.csv\",\n",
    "                \"sscs/\" + songname + \"_S.csv\",\n",
    "                \"sscs/\" + songname + \"_A.csv\",\n",
    "                \"sscs/\" + songname + \"_T.csv\",\n",
    "                \"sscs/\" + songname + \"_B.csv\"]\n",
    "        df = []\n",
    "        for i in range(len(fnames)):\n",
    "            with zf.open(fnames[i]) as f:\n",
    "                df.append(pd.read_csv(f, header=None, engine='pyarrow').T.astype('float32'))\n",
    "        return df\n",
    "    \n",
    "def df_to_hdf5(df, savename, keyname):\n",
    "    df.to_hdf(savename, keyname, mode='a',\n",
    "            format='table', complevel=9, complib='blosc')\n",
    "\n",
    "@ray.remote(max_retries=-1)  \n",
    "def csv_to_hdf5(songname, songname_conformity, split='train'):\n",
    "    keynames = ['mix', 'soprano', 'alto',\n",
    "                'tenor', 'bass']\n",
    "    savename = h5_pathname + \"Files/\" + songname_conformity + \".h5\"\n",
    "    try:\n",
    "        if(not os.path.exists(savename)):\n",
    "            df = csv_to_df(songname)\n",
    "            for i in range(len(keynames)):\n",
    "                df_to_hdf5(df[i], savename, keynames[i])\n",
    "    except:\n",
    "        with open(h5_pathname + \"Problematic_Songs.txt\", \"a\") as f:\n",
    "            f.write(songname + \"\\n\")\n",
    "        if(os.path.exists(savename)):\n",
    "            os.remove(savename)\n",
    "\n",
    "        try:\n",
    "            train.remove(songname)\n",
    "            train_conformity.remove(songname_conformity)\n",
    "        except:\n",
    "            try:\n",
    "                validate.remove(songname)\n",
    "                validate_conformity.remove(songname_conformity)\n",
    "            except:\n",
    "                try:\n",
    "                    test.remove(songname)\n",
    "                    test_conformity.remove(songname_conformity)\n",
    "                except:\n",
    "                    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "conv_train = [csv_to_hdf5.remote(train[i],\n",
    "    train_conformity[i]) for i in range(3200, len(train))]\n",
    "conv_train_get = ray.get(conv_train)\n",
    "\n",
    "conv_val = [csv_to_hdf5.remote(validate[i],\n",
    "    validate_conformity[i], \"validate\") for i in range(len(validate))]\n",
    "conv_val_get = ray.get(conv_val)\n",
    "\n",
    "conv_test = [csv_to_hdf5.remote(test[i],\n",
    "    test_conformity[i], \"test\") for i in range(len(test))]\n",
    "conv_test_get = ray.get(conv_test)\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor song in hdf5_metadata[\\'train\\']:\\n    if not (song in hdf5_converted):\\n        hdf5_metadata[\\'train\\'].remove(song)\\n\\nfor song in hdf5_metadata[\\'validate\\']:\\n    if not (song in hdf5_converted):\\n        hdf5_metadata[\\'validate\\'].remove(song)\\n\\nfor song in hdf5_metadata[\\'test\\']:\\n    if not (song in hdf5_converted):\\n        hdf5_metadata[\\'test\\'].remove(song)\\n\\nmetadata_filename = h5_pathname + \"SynthSalienceChoralSet_hdf5_dataSplits.json\"\\nwith open(metadata_filename, \"w\") as metadata_file:\\n    json.dump(hdf5_metadata, metadata_file, indent=4)\\n\\n#len(original_files) - len(problematic_files)\\n#len(hdf5_metadata[\\'train\\']) + len(hdf5_metadata[\\'validate\\']) + len(hdf5_metadata[\\'test\\'])\\n'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdf5_converted = [os.path.splitext(f)[0] for f in os.listdir(h5_pathname + \"Files/\")]\n",
    "original_files = []\n",
    "original_files.extend(name_conformity(i) for i in sscs_get_split())\n",
    "original_files.extend(name_conformity(i) for i in sscs_get_split('validate'))\n",
    "original_files.extend(name_conformity(i) for i in sscs_get_split('test'))\n",
    "problematic_files = []\n",
    "problematic_files.extend(original_files)\n",
    "\n",
    "for song in hdf5_converted:\n",
    "    if song in problematic_files:\n",
    "        problematic_files.remove(song)\n",
    "\n",
    "with open(h5_pathname + \"Problematic_Songs.txt\", \"w\") as f:\n",
    "    f.writelines([i + \"\\n\" for i in problematic_files])\n",
    "\n",
    "\n",
    "hdf5_metadata = {}\n",
    "hdf5_metadata['train'] = [name_conformity(i) for i in sscs_get_split()]\n",
    "hdf5_metadata['validate'] = [name_conformity(i) for i in sscs_get_split('validate')]\n",
    "hdf5_metadata['test'] = [name_conformity(i) for i in sscs_get_split('test')]\n",
    "'''\n",
    "for song in hdf5_metadata['train']:\n",
    "    if not (song in hdf5_converted):\n",
    "        hdf5_metadata['train'].remove(song)\n",
    "\n",
    "for song in hdf5_metadata['validate']:\n",
    "    if not (song in hdf5_converted):\n",
    "        hdf5_metadata['validate'].remove(song)\n",
    "\n",
    "for song in hdf5_metadata['test']:\n",
    "    if not (song in hdf5_converted):\n",
    "        hdf5_metadata['test'].remove(song)\n",
    "\n",
    "metadata_filename = h5_pathname + \"SynthSalienceChoralSet_hdf5_dataSplits.json\"\n",
    "with open(metadata_filename, \"w\") as metadata_file:\n",
    "    json.dump(hdf5_metadata, metadata_file, indent=4)\n",
    "\n",
    "#len(original_files) - len(problematic_files)\n",
    "#len(hdf5_metadata['train']) + len(hdf5_metadata['validate']) + len(hdf5_metadata['test'])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_metadata = {}\n",
    "hdf5_metadata['train'] = train_conformity\n",
    "hdf5_metadata['validate'] = validate_conformity\n",
    "hdf5_metadata['test'] = test_conformity\n",
    "\n",
    "metadata_filename = h5_pathname + \"SynthSalienceChoralSet_hdf5_dataSplits.json\"\n",
    "with open(metadata_filename, \"w\") as metadata_file:\n",
    "    json.dump(hdf5_metadata, metadata_file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
