{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPqelqw7u6NR"
      },
      "source": [
        "# **SSCS - Test Playground - tf.data**\n",
        "\n",
        "Developed by André Paiva\n",
        "\n",
        "Based on SSCS Dataset created by Helena Cuesta and Emilia Gómez"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iusMrIYiu6NW"
      },
      "outputs": [],
      "source": [
        "EXECUTE_ON_COLAB = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9JbTeigu6NY",
        "outputId": "202f4bd1-8c57-4cfb-ec2a-180e7c374a25"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import h5py\n",
        "import json\n",
        "import time\n",
        "import zipfile\n",
        "import tables\n",
        "import requests\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import BinaryCrossentropy, CategoricalCrossentropy\n",
        "from tensorflow.keras.metrics import Accuracy, Precision\n",
        "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization\n",
        "\n",
        "if(EXECUTE_ON_COLAB):\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCimfZ6Wu6NZ",
        "outputId": "5c85601e-00d9-4402-aada-997049f6d419"
      },
      "outputs": [],
      "source": [
        "tf.config.list_physical_devices('GPU')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhwwUoapu6Na"
      },
      "source": [
        "## 2 - Neural Network Model (VoasCNN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EtBxJ6AIu6Nb"
      },
      "outputs": [],
      "source": [
        "SAVE_MODEL = True\n",
        "LOAD_MODEL = True\n",
        "SPLIT_SIZE = 128\n",
        "BATCH_SIZE = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gz4HHbLQu6Nc"
      },
      "outputs": [],
      "source": [
        "def voas_cnn_model():\n",
        "    x_in = Input(shape=(360, SPLIT_SIZE, 1))\n",
        "    \n",
        "    x = BatchNormalization()(x_in)\n",
        "\n",
        "    x = Conv2D(filters=32, kernel_size=(3, 3), padding=\"same\",\n",
        "        activation=\"relu\", name=\"conv1\")(x)\n",
        "\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    x = Conv2D(filters=32, kernel_size=(3, 3), padding=\"same\",\n",
        "        activation=\"relu\", name=\"conv2\")(x)\n",
        "\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    x = Conv2D(filters=16, kernel_size=(70, 3), padding=\"same\",\n",
        "        activation=\"relu\", name=\"conv_harm_1\")(x)\n",
        "\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    x = Conv2D(filters=16, kernel_size=(70, 3), padding=\"same\",\n",
        "        activation=\"relu\", name=\"conv_harm_2\")(x)\n",
        "\n",
        "    ## start four branches now\n",
        "\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    ## branch 1\n",
        "    x1a = Conv2D(filters=16, kernel_size=(3, 3), padding=\"same\",\n",
        "        activation=\"relu\", name=\"conv1a\")(x)\n",
        "\n",
        "    x1a = BatchNormalization()(x1a)\n",
        "\n",
        "    x1b = Conv2D(filters=16, kernel_size=(3, 3), padding=\"same\",\n",
        "        activation=\"relu\", name=\"conv1b\")(x1a)\n",
        "\n",
        "    ## branch 2\n",
        "    x2a = Conv2D(filters=16, kernel_size=(3, 3), padding=\"same\",\n",
        "        activation=\"relu\", name=\"conv2a\")(x)\n",
        "\n",
        "    x2a = BatchNormalization()(x2a)\n",
        "\n",
        "    x2b = Conv2D(filters=16, kernel_size=(3, 3), padding=\"same\",\n",
        "        activation=\"relu\", name=\"conv2b\")(x2a)\n",
        "\n",
        "    ## branch 3\n",
        "\n",
        "    x3a = Conv2D(filters=16, kernel_size=(3, 3), padding=\"same\",\n",
        "        activation=\"relu\", name=\"conv3a\")(x)\n",
        "\n",
        "    x3a = BatchNormalization()(x3a)\n",
        "\n",
        "    x3b = Conv2D(filters=16, kernel_size=(3, 3), padding=\"same\",\n",
        "        activation=\"relu\", name=\"conv3b\")(x3a)\n",
        "\n",
        "    x4a = Conv2D(filters=16, kernel_size=(3, 3), padding=\"same\",\n",
        "        activation=\"relu\", name=\"conv4a\")(x)\n",
        "\n",
        "    x4a = BatchNormalization()(x4a)\n",
        "\n",
        "    x4b = Conv2D(filters=16, kernel_size=(3, 3), padding=\"same\",\n",
        "        activation=\"relu\", name=\"conv4b\"\n",
        "    )(x4a)\n",
        "\n",
        "\n",
        "    y1 = Conv2D(filters=1, kernel_size=1, name='conv_soprano',\n",
        "                padding='same', activation='sigmoid')(x1b)\n",
        "    y1 = tf.squeeze(y1, axis=-1, name='sop')\n",
        "    y2 = Conv2D(filters=1, kernel_size=1, name='conv_alto',\n",
        "                padding='same', activation='sigmoid')(x2b)\n",
        "    y2 = tf.squeeze(y2, axis=-1, name='alt')\n",
        "    y3 = Conv2D(filters=1, kernel_size=1, name='conv_tenor',\n",
        "                padding='same', activation='sigmoid')(x3b)\n",
        "    y3 = tf.squeeze(y3, axis=-1, name='ten')\n",
        "    y4 = Conv2D(filters=1, kernel_size=1, name='conv_bass',\n",
        "                padding='same', activation='sigmoid')(x4b)\n",
        "    y4 = tf.squeeze(y4, axis=-1, name='bas')\n",
        "\n",
        "    out = [y1, y2, y3, y4]\n",
        "\n",
        "    model = Model(inputs=x_in, outputs=out, name='voasCNN')\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fV2oGV_3u6Ne"
      },
      "source": [
        "## 3 - Auxiliar functions and Variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_V5JoYGu6Ne"
      },
      "source": [
        "Functions designed to manipulate the SSCS dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_drR4LKXu6Nf"
      },
      "source": [
        "### 3.1 - File path variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpy6f5fVu6Nf"
      },
      "outputs": [],
      "source": [
        "if(EXECUTE_ON_COLAB):\n",
        "    dataset_dir = \"/content/Datasets/\"\n",
        "    checkpoint_dir = \"/content/drive/MyDrive/SSCS/Checkpoints/sscs.ckpt\"\n",
        "else:\n",
        "    dataset_dir = \"Datasets/\"\n",
        "    checkpoint_dir = \"Checkpoints/sscs.ckpt\"\n",
        "zipname = dataset_dir + \"SSCS_HDF5.zip\"\n",
        "sscs_dir = dataset_dir + \"SSCS_HDF5/\"\n",
        "\n",
        "songs_dir = sscs_dir + \"sscs/\"\n",
        "splitname = sscs_dir + \"sscs_splits.json\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nc_YSVBdu6Ng"
      },
      "source": [
        "### 3.2 - Download/Extract Scripts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kbQ7sMjRu6Nh"
      },
      "outputs": [],
      "source": [
        "def download(url, fname):\n",
        "    \n",
        "    resp = requests.get(url, stream=True)\n",
        "    total = int(resp.headers.get('content-length', 0))\n",
        "    downloaded_size = 0\n",
        "    with open(fname, 'wb') as file:\n",
        "        for data in resp.iter_content(chunk_size=max(4096, int(total/10000))):\n",
        "            size = file.write(data)\n",
        "            downloaded_size += size\n",
        "            percent = min(downloaded_size/total, 1.0)\n",
        "            print(f\"\\r{percent:.2%} downloaded\", end='')\n",
        "            \n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5yWY3OBzu6Ni"
      },
      "outputs": [],
      "source": [
        "def sscs_download():\n",
        "    \n",
        "    if(not os.path.exists(dataset_dir)):\n",
        "        os.mkdir(dataset_dir)\n",
        "   \n",
        "    if(not os.path.exists(zipname)):\n",
        "        print(\"Downloading SSCS Dataset...\")\n",
        "        url = \"https://github.com/Xornotor/SSCS_HDF5/releases/download/v1.0/SSCS_HDF5.zip\"\n",
        "        download(url, zipname)\n",
        "    else:\n",
        "        print(\"SSCS Dataset found.\")\n",
        "\n",
        "    if(not os.path.exists(sscs_dir)):\n",
        "        print(\"Extracting SSCS Dataset...\")\n",
        "        with zipfile.ZipFile(zipname) as zf:\n",
        "            os.mkdir(sscs_dir)\n",
        "            zf.extractall(path=sscs_dir)\n",
        "    else:\n",
        "        print(\"SSCS Dataset already extracted.\")\n",
        "    \n",
        "    print(\"Done.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMy5TXSeu6Ni"
      },
      "source": [
        "### 3.3 - Splits, songnames and songlists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "euVHDE5Gu6Nj"
      },
      "outputs": [],
      "source": [
        "def sscs_get_split(split='train'):\n",
        "    \n",
        "    if(split.lower() == 'train' or split.lower() == 'validate' or\n",
        "       split.lower() == 'test'):\n",
        "        split_list = json.load(open(splitname, 'r'))[split.lower()]\n",
        "        return split_list\n",
        "    else:\n",
        "        raise NameError(\"Split should be 'train', 'validate' or 'test'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0swxeZiu6Nj"
      },
      "outputs": [],
      "source": [
        "def sscs_pick_songlist(first=0, amount=5, split='train'):\n",
        "    \n",
        "    songnames = sscs_get_split(split)\n",
        "    return songnames[first:first+amount]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZ1-RwvUu6Nk"
      },
      "outputs": [],
      "source": [
        "def sscs_pick_random_song(split='train'):\n",
        "    \n",
        "    songnames = sscs_get_split(split)\n",
        "    rng = np.random.randint(0, len(songnames))\n",
        "    return songnames[rng]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QvBd6aH6u6Nk"
      },
      "outputs": [],
      "source": [
        "def sscs_pick_multiple_random_songs(amount, split='train'):\n",
        "    \n",
        "    return [sscs_pick_random_song() for i in range(amount)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2agnHW-qu6Nl"
      },
      "source": [
        "### 3.4 - Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SbZGhToDu6Nl"
      },
      "outputs": [],
      "source": [
        "def sscs_plot(dataframe):\n",
        "\n",
        "    aspect_ratio = (3/8)*dataframe.shape[1]/dataframe.shape[0]\n",
        "    fig, ax = plt.subplots(figsize=(13, 7))\n",
        "    im = ax.imshow(dataframe, interpolation='nearest', aspect=aspect_ratio,\n",
        "        cmap = mpl.colormaps['BuPu'])\n",
        "    ax.invert_yaxis()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIXMHFZju6Nl"
      },
      "source": [
        "## 4 - Download and extract dataset SSCS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjQMk_4pu6Nm",
        "outputId": "0dd6b5b2-6cfc-4678-d01b-3a4687f560c4"
      },
      "outputs": [],
      "source": [
        "sscs_download()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnfG99TPu6Nn"
      },
      "source": [
        "## 5 - Dataset Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37Bo9cpbu6Nn"
      },
      "outputs": [],
      "source": [
        "class SSCS_Sequence(tf.keras.utils.Sequence):\n",
        "    \n",
        "    ############################################################\n",
        "\n",
        "    def __init__(self, filenames, batch_size=BATCH_SIZE, split_size=SPLIT_SIZE):\n",
        "\n",
        "        if(isinstance(filenames, np.ndarray)):\n",
        "            self.filenames = [f.decode('utf-8') for f in filenames.tolist()]\n",
        "        else:\n",
        "            self.filenames = filenames\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.batches_amount = 0\n",
        "        self.splits_per_file = np.array([], dtype=np.intc)\n",
        "        self.songs_dir = songs_dir\n",
        "        self.split_size = split_size\n",
        "        self.idx_get = np.array([], dtype=np.intc)\n",
        "        self.split_get = np.array([], dtype=np.intc)\n",
        "        self.debug = True\n",
        "\n",
        "        for file in self.filenames:\n",
        "\n",
        "            file_access = f\"{self.songs_dir}{file}.h5\"\n",
        "            f = h5py.File(file_access, 'r')\n",
        "            file_shape = f['mix/table'].shape[0]\n",
        "            df_batch_items = file_shape//self.split_size\n",
        "            #if(file_shape/self.split_size > df_batch_items): df_batch_items += 1\n",
        "            self.splits_per_file = np.append(self.splits_per_file, int(df_batch_items))\n",
        "            tmp_idx_get = np.array([self.filenames.index(file) for i in range(df_batch_items)], dtype=np.intc)\n",
        "            tmp_split_get = np.array([i for i in range(df_batch_items)], dtype=np.intc)\n",
        "            self.idx_get = np.append(self.idx_get, tmp_idx_get)\n",
        "            self.split_get = np.append(self.split_get, tmp_split_get)\n",
        "            f.close()\n",
        "        \n",
        "        self.batches_amount = self.split_get.shape[0]//self.batch_size\n",
        "        if self.batches_amount < self.split_get.shape[0]/self.batch_size: \n",
        "            self.batches_amount += 1\n",
        "\n",
        "        self.idx_get = np.resize(self.idx_get, self.batches_amount * self.batch_size)\n",
        "        self.idx_get = np.reshape(self.idx_get, (-1, self.batch_size))\n",
        "\n",
        "        self.split_get = np.resize(self.split_get, self.batches_amount * self.batch_size)\n",
        "        self.split_get = np.reshape(self.split_get, (-1, self.batch_size))\n",
        "     \n",
        "    ############################################################\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        return self.batches_amount\n",
        "    \n",
        "    ############################################################\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        if(self.debug):\n",
        "            time_init = time.time()\n",
        "\n",
        "        tmp_idx = self.idx_get[idx]\n",
        "        tmp_split = self.split_get[idx]\n",
        "\n",
        "        batch_splits = np.array(list(map(self.get_split, tmp_idx, tmp_split)))\n",
        "\n",
        "        splits = [tf.convert_to_tensor(batch_splits[:, i], dtype=tf.float32) for i in range(5)]\n",
        "\n",
        "        if(self.debug):\n",
        "            time_end = time.time()\n",
        "            time_interval = time_end - time_init\n",
        "            print(time_interval)\n",
        "\n",
        "        return splits[0], (splits[1], splits[2], splits[3], splits[4]) # mix, (s, a, t, b)\n",
        "    \n",
        "    ############################################################\n",
        "    \n",
        "    def get_split(self, idx, split):\n",
        "\n",
        "        file_access = f\"{self.songs_dir}{self.filenames[idx]}.h5\"\n",
        "        data_min = split * self.split_size\n",
        "        data_max = data_min + self.split_size\n",
        "        voices = ['mix', 'soprano', 'alto', 'tenor', 'bass']\n",
        "\n",
        "        def read_split(voice):\n",
        "\n",
        "            f = h5py.File(file_access, 'r')\n",
        "\n",
        "            data = np.transpose(np.array([line[1] for line in f[voice + \"/table\"][data_min:data_max]]))\n",
        "            data = data.reshape((data.shape[0], data.shape[1], 1))\n",
        "\n",
        "            f.close()\n",
        "\n",
        "            return data\n",
        "\n",
        "        splits = list(map(read_split, voices))\n",
        "\n",
        "        return splits # mix, soprano, alto, tenor, bass\n",
        "    \n",
        "    ############################################################\n",
        "\n",
        "    def get_splits_per_file(self):\n",
        "        \n",
        "        return self.splits_per_file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQwK19nOu6No"
      },
      "outputs": [],
      "source": [
        "seq = SSCS_Sequence(sscs_get_split()[30:60])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwJsJSObu6No",
        "outputId": "ef0cc26c-e247-42c8-9690-bb5291c43b08"
      },
      "outputs": [],
      "source": [
        "seq.__len__()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        },
        "id": "ucShUHwKMeSR",
        "outputId": "74128fab-83b3-49fd-e783-72b79dbce81d"
      },
      "outputs": [],
      "source": [
        "mix_test, satb_test = seq.__getitem__(2)\n",
        "\n",
        "\n",
        "sscs_plot(mix_test.numpy()[28])\n",
        "sscs_plot(satb_test[0].numpy()[28])\n",
        "sscs_plot(satb_test[1].numpy()[28])\n",
        "sscs_plot(satb_test[2].numpy()[28])\n",
        "sscs_plot(satb_test[3].numpy()[28])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoVmVETou6Np"
      },
      "source": [
        "## 6 - Training VoasCNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwxfnsTTu6Np"
      },
      "outputs": [],
      "source": [
        "dsSpec = tf.TensorSpec(shape=mix_test.shape, dtype=tf.float32)\n",
        "\n",
        "signature = (dsSpec, (dsSpec, dsSpec, dsSpec, dsSpec))\n",
        "\n",
        "ds = tf.data.Dataset.from_generator(SSCS_Sequence,\n",
        "                                    args = [sscs_get_split()[30:60]],\n",
        "                                    output_signature=signature\n",
        "                                    ).prefetch(tf.data.AUTOTUNE)\n",
        "                                    #).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "#tf.data.DatasetSpec.from_value(ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCm9s_bsu6Np"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "class VoasCrossentropy(tf.keras.losses.Loss):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "    def call(self, y_true, y_pred):\n",
        "        time_init = time.time()\n",
        "        y_pred_sq = tf.squeeze(y_pred)\n",
        "        y_true_sq = tf.squeeze(y_true)     \n",
        "        elements =  -tf.math.multiply_no_nan(x=tf.math.log(y_pred_sq),\n",
        "                                        y=y_true_sq) \\\n",
        "                    -tf.math.multiply_no_nan(x=tf.math.log(1 - y_pred_sq),\n",
        "                                        y=(1 - y_true_sq))\n",
        "        print(time.time()-time_init)\n",
        "        return tf.reduce_mean(tf.reduce_sum(elements, range(tf.rank(y_pred_sq))))\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwxeIFWNu6Nq"
      },
      "outputs": [],
      "source": [
        "voas_cnn = voas_cnn_model()\n",
        "voas_cnn.compile(optimizer=Adam(learning_rate=5e-3),\n",
        "                 loss=BinaryCrossentropy(reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE),\n",
        "                 metrics=[Precision()])\n",
        "#voas_cnn.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "NS_REu2Vu6Nq",
        "outputId": "f471e9e7-38e8-4e0e-948d-ca9bbe2318a2"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "if(os.path.exists(checkpoint_dir)):\n",
        "    voas_cnn.load_weights(checkpoint_dir)\n",
        "\n",
        "save_cb = tf.keras.callbacks.ModelCheckpoint(   filepath=checkpoint_dir,\n",
        "                                                save_weights_only=True,\n",
        "                                                verbose=1\n",
        "                                            )\n",
        "\n",
        "voas_cnn.fit(ds, epochs=3, callbacks=[save_cb])\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.22254681587219238\n",
            "0.20105218887329102\n",
            "0.253584623336792\n",
            "0.2060701847076416\n",
            "0.20604944229125977\n",
            "0.20754194259643555\n",
            "0.19203996658325195\n",
            "0.21556305885314941\n",
            "0.25304698944091797\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m voas_cnn\u001b[39m.\u001b[39;49mfit(ds, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n",
            "File \u001b[1;32me:\\Programas\\Anaconda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32me:\\Programas\\Anaconda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py:1570\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1568\u001b[0m logs \u001b[39m=\u001b[39m tmp_logs\n\u001b[0;32m   1569\u001b[0m end_step \u001b[39m=\u001b[39m step \u001b[39m+\u001b[39m data_handler\u001b[39m.\u001b[39mstep_increment\n\u001b[1;32m-> 1570\u001b[0m callbacks\u001b[39m.\u001b[39;49mon_train_batch_end(end_step, logs)\n\u001b[0;32m   1571\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop_training:\n\u001b[0;32m   1572\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
            "File \u001b[1;32me:\\Programas\\Anaconda\\envs\\tf\\lib\\site-packages\\keras\\callbacks.py:470\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[0;32m    464\u001b[0m \n\u001b[0;32m    465\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m    466\u001b[0m \u001b[39m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[0;32m    467\u001b[0m \u001b[39m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[0;32m    468\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    469\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[1;32m--> 470\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_hook(ModeKeys\u001b[39m.\u001b[39;49mTRAIN, \u001b[39m\"\u001b[39;49m\u001b[39mend\u001b[39;49m\u001b[39m\"\u001b[39;49m, batch, logs\u001b[39m=\u001b[39;49mlogs)\n",
            "File \u001b[1;32me:\\Programas\\Anaconda\\envs\\tf\\lib\\site-packages\\keras\\callbacks.py:317\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[1;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[0;32m    315\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[0;32m    316\u001b[0m \u001b[39melif\u001b[39;00m hook \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mend\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 317\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_end_hook(mode, batch, logs)\n\u001b[0;32m    318\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    319\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    320\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized hook: \u001b[39m\u001b[39m{\u001b[39;00mhook\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mExpected values are [\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbegin\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mend\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m]\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n",
            "File \u001b[1;32me:\\Programas\\Anaconda\\envs\\tf\\lib\\site-packages\\keras\\callbacks.py:340\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[1;34m(self, mode, batch, logs)\u001b[0m\n\u001b[0;32m    337\u001b[0m     batch_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_start_time\n\u001b[0;32m    338\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_times\u001b[39m.\u001b[39mappend(batch_time)\n\u001b[1;32m--> 340\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_hook_helper(hook_name, batch, logs)\n\u001b[0;32m    342\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_times) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_batches_for_timing_check:\n\u001b[0;32m    343\u001b[0m     end_hook_name \u001b[39m=\u001b[39m hook_name\n",
            "File \u001b[1;32me:\\Programas\\Anaconda\\envs\\tf\\lib\\site-packages\\keras\\callbacks.py:388\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[1;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks:\n\u001b[0;32m    387\u001b[0m     hook \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(callback, hook_name)\n\u001b[1;32m--> 388\u001b[0m     hook(batch, logs)\n\u001b[0;32m    390\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_timing:\n\u001b[0;32m    391\u001b[0m     \u001b[39mif\u001b[39;00m hook_name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hook_times:\n",
            "File \u001b[1;32me:\\Programas\\Anaconda\\envs\\tf\\lib\\site-packages\\keras\\callbacks.py:1081\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1080\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mon_train_batch_end\u001b[39m(\u001b[39mself\u001b[39m, batch, logs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m-> 1081\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_update_progbar(batch, logs)\n",
            "File \u001b[1;32me:\\Programas\\Anaconda\\envs\\tf\\lib\\site-packages\\keras\\callbacks.py:1157\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1153\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseen \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m add_seen\n\u001b[0;32m   1155\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1156\u001b[0m     \u001b[39m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[1;32m-> 1157\u001b[0m     logs \u001b[39m=\u001b[39m tf_utils\u001b[39m.\u001b[39;49msync_to_numpy_or_python_type(logs)\n\u001b[0;32m   1158\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprogbar\u001b[39m.\u001b[39mupdate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseen, \u001b[39mlist\u001b[39m(logs\u001b[39m.\u001b[39mitems()), finalize\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
            "File \u001b[1;32me:\\Programas\\Anaconda\\envs\\tf\\lib\\site-packages\\keras\\utils\\tf_utils.py:635\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    632\u001b[0m         \u001b[39mreturn\u001b[39;00m t\n\u001b[0;32m    633\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mitem() \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39mndim(t) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m t\n\u001b[1;32m--> 635\u001b[0m \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39;49mnest\u001b[39m.\u001b[39;49mmap_structure(_to_single_numpy_or_python_type, tensors)\n",
            "File \u001b[1;32me:\\Programas\\Anaconda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:917\u001b[0m, in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    913\u001b[0m flat_structure \u001b[39m=\u001b[39m (flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[0;32m    914\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[0;32m    916\u001b[0m \u001b[39mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 917\u001b[0m     structure[\u001b[39m0\u001b[39m], [func(\u001b[39m*\u001b[39mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[0;32m    918\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites)\n",
            "File \u001b[1;32me:\\Programas\\Anaconda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:917\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    913\u001b[0m flat_structure \u001b[39m=\u001b[39m (flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[0;32m    914\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[0;32m    916\u001b[0m \u001b[39mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 917\u001b[0m     structure[\u001b[39m0\u001b[39m], [func(\u001b[39m*\u001b[39;49mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[0;32m    918\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites)\n",
            "File \u001b[1;32me:\\Programas\\Anaconda\\envs\\tf\\lib\\site-packages\\keras\\utils\\tf_utils.py:628\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[0;32m    626\u001b[0m     \u001b[39m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(t, tf\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m--> 628\u001b[0m         t \u001b[39m=\u001b[39m t\u001b[39m.\u001b[39;49mnumpy()\n\u001b[0;32m    629\u001b[0m     \u001b[39m# Strings, ragged and sparse tensors don't have .item(). Return them\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[39m# as-is.\u001b[39;00m\n\u001b[0;32m    631\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(t, (np\u001b[39m.\u001b[39mndarray, np\u001b[39m.\u001b[39mgeneric)):\n",
            "File \u001b[1;32me:\\Programas\\Anaconda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1157\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1134\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m \n\u001b[0;32m   1136\u001b[0m \u001b[39mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1154\u001b[0m \u001b[39m    NumPy dtype.\u001b[39;00m\n\u001b[0;32m   1155\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1156\u001b[0m \u001b[39m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[1;32m-> 1157\u001b[0m maybe_arr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_numpy()  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1158\u001b[0m \u001b[39mreturn\u001b[39;00m maybe_arr\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(maybe_arr, np\u001b[39m.\u001b[39mndarray) \u001b[39melse\u001b[39;00m maybe_arr\n",
            "File \u001b[1;32me:\\Programas\\Anaconda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1123\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1121\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_numpy\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m   1122\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_numpy_internal()\n\u001b[0;32m   1124\u001b[0m   \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1125\u001b[0m     \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_status_to_exception(e) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "voas_cnn.fit(ds, epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "B2MZ_3ghu6Nq",
        "outputId": "6c700708-f12a-4682-fd4e-b108ebae4493"
      },
      "outputs": [],
      "source": [
        "mix, satb = seq.__getitem__(np.random.randint(0, seq.__len__() - 1))\n",
        "\n",
        "mix = mix.numpy()\n",
        "\n",
        "s = satb[0].numpy()\n",
        "a = satb[1].numpy()\n",
        "t = satb[2].numpy()\n",
        "b = satb[3].numpy()\n",
        "\n",
        "s_pred, a_pred, t_pred, b_pred = voas_cnn.predict(mix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "BsAHWtRlu6Nr",
        "outputId": "00f9291f-686a-46c9-a5a4-7a53d05c4cfe"
      },
      "outputs": [],
      "source": [
        "idx = 10\n",
        "\n",
        "sscs_plot(a[idx])\n",
        "sscs_plot(a_pred[idx])\n",
        "sscs_plot(mix[idx])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
