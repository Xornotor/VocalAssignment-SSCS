{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPqelqw7u6NR"
      },
      "source": [
        "# **SSCS - Test Playground - tf.data**\n",
        "\n",
        "Developed by André Paiva\n",
        "\n",
        "Based on SSCS Dataset created by Helena Cuesta and Emilia Gómez"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iusMrIYiu6NW"
      },
      "outputs": [],
      "source": [
        "EXECUTE_ON_COLAB = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9JbTeigu6NY",
        "outputId": "202f4bd1-8c57-4cfb-ec2a-180e7c374a25"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import h5py\n",
        "import json\n",
        "import time\n",
        "import zipfile\n",
        "import tables\n",
        "import requests\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import BinaryCrossentropy, CategoricalCrossentropy\n",
        "from tensorflow.keras.metrics import Accuracy, Precision\n",
        "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization\n",
        "\n",
        "if(EXECUTE_ON_COLAB):\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCimfZ6Wu6NZ",
        "outputId": "5c85601e-00d9-4402-aada-997049f6d419"
      },
      "outputs": [],
      "source": [
        "tf.config.list_physical_devices('GPU')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhwwUoapu6Na"
      },
      "source": [
        "## 2 - Neural Network Model (VoasCNN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EtBxJ6AIu6Nb"
      },
      "outputs": [],
      "source": [
        "SAVE_MODEL = True\n",
        "LOAD_MODEL = True\n",
        "SPLIT_SIZE = 128\n",
        "BATCH_SIZE = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gz4HHbLQu6Nc"
      },
      "outputs": [],
      "source": [
        "def voas_cnn_model():\n",
        "    x_in = Input(shape=(360, SPLIT_SIZE, 1))\n",
        "    \n",
        "    x = BatchNormalization()(x_in)\n",
        "\n",
        "    x = Conv2D(filters=32, kernel_size=(3, 3), padding=\"same\",\n",
        "        activation=\"relu\", name=\"conv1\")(x)\n",
        "\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    x = Conv2D(filters=32, kernel_size=(3, 3), padding=\"same\",\n",
        "        activation=\"relu\", name=\"conv2\")(x)\n",
        "\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    x = Conv2D(filters=16, kernel_size=(70, 3), padding=\"same\",\n",
        "        activation=\"relu\", name=\"conv_harm_1\")(x)\n",
        "\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    x = Conv2D(filters=16, kernel_size=(70, 3), padding=\"same\",\n",
        "        activation=\"relu\", name=\"conv_harm_2\")(x)\n",
        "\n",
        "    ## start four branches now\n",
        "\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    ## branch 1\n",
        "    x1a = Conv2D(filters=16, kernel_size=(3, 3), padding=\"same\",\n",
        "        activation=\"relu\", name=\"conv1a\")(x)\n",
        "\n",
        "    x1a = BatchNormalization()(x1a)\n",
        "\n",
        "    x1b = Conv2D(filters=16, kernel_size=(3, 3), padding=\"same\",\n",
        "        activation=\"relu\", name=\"conv1b\")(x1a)\n",
        "\n",
        "    ## branch 2\n",
        "    x2a = Conv2D(filters=16, kernel_size=(3, 3), padding=\"same\",\n",
        "        activation=\"relu\", name=\"conv2a\")(x)\n",
        "\n",
        "    x2a = BatchNormalization()(x2a)\n",
        "\n",
        "    x2b = Conv2D(filters=16, kernel_size=(3, 3), padding=\"same\",\n",
        "        activation=\"relu\", name=\"conv2b\")(x2a)\n",
        "\n",
        "    ## branch 3\n",
        "\n",
        "    x3a = Conv2D(filters=16, kernel_size=(3, 3), padding=\"same\",\n",
        "        activation=\"relu\", name=\"conv3a\")(x)\n",
        "\n",
        "    x3a = BatchNormalization()(x3a)\n",
        "\n",
        "    x3b = Conv2D(filters=16, kernel_size=(3, 3), padding=\"same\",\n",
        "        activation=\"relu\", name=\"conv3b\")(x3a)\n",
        "\n",
        "    x4a = Conv2D(filters=16, kernel_size=(3, 3), padding=\"same\",\n",
        "        activation=\"relu\", name=\"conv4a\")(x)\n",
        "\n",
        "    x4a = BatchNormalization()(x4a)\n",
        "\n",
        "    x4b = Conv2D(filters=16, kernel_size=(3, 3), padding=\"same\",\n",
        "        activation=\"relu\", name=\"conv4b\"\n",
        "    )(x4a)\n",
        "\n",
        "\n",
        "    y1 = Conv2D(filters=1, kernel_size=1, name='conv_soprano',\n",
        "                padding='same', activation='sigmoid')(x1b)\n",
        "    y1 = tf.squeeze(y1, axis=-1, name='sop')\n",
        "    y2 = Conv2D(filters=1, kernel_size=1, name='conv_alto',\n",
        "                padding='same', activation='sigmoid')(x2b)\n",
        "    y2 = tf.squeeze(y2, axis=-1, name='alt')\n",
        "    y3 = Conv2D(filters=1, kernel_size=1, name='conv_tenor',\n",
        "                padding='same', activation='sigmoid')(x3b)\n",
        "    y3 = tf.squeeze(y3, axis=-1, name='ten')\n",
        "    y4 = Conv2D(filters=1, kernel_size=1, name='conv_bass',\n",
        "                padding='same', activation='sigmoid')(x4b)\n",
        "    y4 = tf.squeeze(y4, axis=-1, name='bas')\n",
        "\n",
        "    out = [y1, y2, y3, y4]\n",
        "\n",
        "    model = Model(inputs=x_in, outputs=out, name='voasCNN')\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fV2oGV_3u6Ne"
      },
      "source": [
        "## 3 - Auxiliar functions and Variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_V5JoYGu6Ne"
      },
      "source": [
        "Functions designed to manipulate the SSCS dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_drR4LKXu6Nf"
      },
      "source": [
        "### 3.1 - File path variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpy6f5fVu6Nf"
      },
      "outputs": [],
      "source": [
        "if(EXECUTE_ON_COLAB):\n",
        "    dataset_dir = \"/content/Datasets/\"\n",
        "    checkpoint_dir = \"/content/drive/MyDrive/SSCS/Checkpoints/sscs.ckpt\"\n",
        "else:\n",
        "    dataset_dir = \"Datasets/\"\n",
        "    checkpoint_dir = \"Checkpoints/sscs.ckpt\"\n",
        "zipname = dataset_dir + \"SSCS_HDF5.zip\"\n",
        "sscs_dir = dataset_dir + \"SSCS_HDF5/\"\n",
        "\n",
        "songs_dir = sscs_dir + \"sscs/\"\n",
        "splitname = sscs_dir + \"sscs_splits.json\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nc_YSVBdu6Ng"
      },
      "source": [
        "### 3.2 - Download/Extract Scripts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kbQ7sMjRu6Nh"
      },
      "outputs": [],
      "source": [
        "def download(url, fname):\n",
        "    \n",
        "    resp = requests.get(url, stream=True)\n",
        "    total = int(resp.headers.get('content-length', 0))\n",
        "    downloaded_size = 0\n",
        "    with open(fname, 'wb') as file:\n",
        "        for data in resp.iter_content(chunk_size=max(4096, int(total/10000))):\n",
        "            size = file.write(data)\n",
        "            downloaded_size += size\n",
        "            percent = min(downloaded_size/total, 1.0)\n",
        "            print(f\"\\r{percent:.2%} downloaded\", end='')\n",
        "            \n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5yWY3OBzu6Ni"
      },
      "outputs": [],
      "source": [
        "def sscs_download():\n",
        "    \n",
        "    if(not os.path.exists(dataset_dir)):\n",
        "        os.mkdir(dataset_dir)\n",
        "   \n",
        "    if(not os.path.exists(zipname)):\n",
        "        print(\"Downloading SSCS Dataset...\")\n",
        "        url = \"https://github.com/Xornotor/SSCS_HDF5/releases/download/v1.0/SSCS_HDF5.zip\"\n",
        "        download(url, zipname)\n",
        "    else:\n",
        "        print(\"SSCS Dataset found.\")\n",
        "\n",
        "    if(not os.path.exists(sscs_dir)):\n",
        "        print(\"Extracting SSCS Dataset...\")\n",
        "        with zipfile.ZipFile(zipname) as zf:\n",
        "            os.mkdir(sscs_dir)\n",
        "            zf.extractall(path=sscs_dir)\n",
        "    else:\n",
        "        print(\"SSCS Dataset already extracted.\")\n",
        "    \n",
        "    print(\"Done.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMy5TXSeu6Ni"
      },
      "source": [
        "### 3.3 - Splits, songnames and songlists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "euVHDE5Gu6Nj"
      },
      "outputs": [],
      "source": [
        "def sscs_get_split(split='train'):\n",
        "    \n",
        "    if(split.lower() == 'train' or split.lower() == 'validate' or\n",
        "       split.lower() == 'test'):\n",
        "        split_list = json.load(open(splitname, 'r'))[split.lower()]\n",
        "        return split_list\n",
        "    else:\n",
        "        raise NameError(\"Split should be 'train', 'validate' or 'test'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0swxeZiu6Nj"
      },
      "outputs": [],
      "source": [
        "def sscs_pick_songlist(first=0, amount=5, split='train'):\n",
        "    \n",
        "    songnames = sscs_get_split(split)\n",
        "    return songnames[first:first+amount]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZ1-RwvUu6Nk"
      },
      "outputs": [],
      "source": [
        "def sscs_pick_random_song(split='train'):\n",
        "    \n",
        "    songnames = sscs_get_split(split)\n",
        "    rng = np.random.randint(0, len(songnames))\n",
        "    return songnames[rng]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QvBd6aH6u6Nk"
      },
      "outputs": [],
      "source": [
        "def sscs_pick_multiple_random_songs(amount, split='train'):\n",
        "    \n",
        "    return [sscs_pick_random_song() for i in range(amount)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2agnHW-qu6Nl"
      },
      "source": [
        "### 3.4 - Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SbZGhToDu6Nl"
      },
      "outputs": [],
      "source": [
        "def sscs_plot(dataframe):\n",
        "\n",
        "    aspect_ratio = (3/8)*dataframe.shape[1]/dataframe.shape[0]\n",
        "    fig, ax = plt.subplots(figsize=(13, 7))\n",
        "    im = ax.imshow(dataframe, interpolation='nearest', aspect=aspect_ratio,\n",
        "        cmap = mpl.colormaps['BuPu'])\n",
        "    ax.invert_yaxis()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIXMHFZju6Nl"
      },
      "source": [
        "## 4 - Download and extract dataset SSCS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjQMk_4pu6Nm",
        "outputId": "0dd6b5b2-6cfc-4678-d01b-3a4687f560c4"
      },
      "outputs": [],
      "source": [
        "sscs_download()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnfG99TPu6Nn"
      },
      "source": [
        "## 5 - Dataset Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37Bo9cpbu6Nn"
      },
      "outputs": [],
      "source": [
        "class SSCS_Sequence(tf.keras.utils.Sequence):\n",
        "    def __init__(self, filenames, batch_size=BATCH_SIZE):\n",
        "\n",
        "        if(isinstance(filenames, np.ndarray)):\n",
        "            self.filenames = [f.decode('utf-8') for f in filenames.tolist()]\n",
        "        else:\n",
        "            self.filenames = filenames\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.batches_amount = 0\n",
        "        self.splits_amount = 0\n",
        "        self.splits_per_file = np.array([], dtype=np.intc)\n",
        "        self.songs_dir = songs_dir\n",
        "        self.split_size = SPLIT_SIZE\n",
        "        self.idx_get = np.array([], dtype=np.intc)\n",
        "        self.split_get = np.array([], dtype=np.intc)\n",
        "        self.debug = False\n",
        "\n",
        "        for file in self.filenames:\n",
        "\n",
        "            file_access = f\"{self.songs_dir}{file}.h5\"\n",
        "            f = h5py.File(file_access, 'r')\n",
        "            file_shape = f['mix/table'].shape[0]\n",
        "            df_batch_items = file_shape//self.split_size\n",
        "            #if(file_shape/self.split_size > df_batch_items): df_batch_items += 1\n",
        "            self.splits_per_file = np.append(self.splits_per_file, int(df_batch_items))\n",
        "            self.splits_amount += df_batch_items\n",
        "            tmp_idx_get = np.array([self.filenames.index(file) for i in range(df_batch_items)], dtype=np.intc)\n",
        "            tmp_split_get = np.array([i for i in range(df_batch_items)], dtype=np.intc)\n",
        "            self.idx_get = np.append(self.idx_get, tmp_idx_get)\n",
        "            self.split_get = np.append(self.split_get, tmp_split_get)\n",
        "            f.close()\n",
        "        \n",
        "        self.batches_amount = self.split_get.shape[0]//self.batch_size\n",
        "        if self.batches_amount < self.split_get.shape[0]/self.batch_size: \n",
        "            self.batches_amount += 1\n",
        "\n",
        "        self.idx_get = np.resize(self.idx_get, self.batches_amount * self.batch_size)\n",
        "        self.idx_get = np.reshape(self.idx_get, (-1, self.batch_size))\n",
        "\n",
        "        self.split_get = np.resize(self.split_get, self.batches_amount * self.batch_size)\n",
        "        self.split_get = np.reshape(self.split_get, (-1, self.batch_size))\n",
        "     \n",
        "    def __len__(self):\n",
        "\n",
        "        return self.batches_amount\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        if(self.debug):\n",
        "            time_init = time.time()\n",
        "\n",
        "        tmp_idx = self.idx_get[idx]\n",
        "        tmp_split = self.split_get[idx]\n",
        "\n",
        "        for i in range(self.batch_size):\n",
        "            file_access = f\"{self.songs_dir}{self.filenames[tmp_idx[i]]}.h5\"\n",
        "            data_min = tmp_split[i] * self.split_size\n",
        "            data_max = data_min + self.split_size\n",
        "\n",
        "            tmp_mix = np.transpose(pd.read_hdf(file_access, 'mix', start=data_min, stop=data_max)).to_numpy()\n",
        "            tmp_s = np.transpose(pd.read_hdf(file_access, 'soprano', start=data_min, stop=data_max)).to_numpy()\n",
        "            tmp_a = np.transpose(pd.read_hdf(file_access, 'alto', start=data_min, stop=data_max)).to_numpy()\n",
        "            tmp_t = np.transpose(pd.read_hdf(file_access, 'tenor', start=data_min, stop=data_max)).to_numpy()\n",
        "            tmp_b = np.transpose(pd.read_hdf(file_access, 'bass', start=data_min, stop=data_max)).to_numpy()\n",
        "\n",
        "            tmp_mix = tmp_mix.reshape((1, tmp_mix.shape[0], tmp_mix.shape[1], 1))\n",
        "            tmp_s = tmp_s.reshape((1, tmp_s.shape[0], tmp_s.shape[1], 1))\n",
        "            tmp_a = tmp_a.reshape((1, tmp_a.shape[0], tmp_a.shape[1], 1))\n",
        "            tmp_t = tmp_t.reshape((1, tmp_t.shape[0], tmp_t.shape[1], 1))\n",
        "            tmp_b = tmp_b.reshape((1, tmp_b.shape[0], tmp_b.shape[1], 1))\n",
        "\n",
        "            if(i == 0):\n",
        "                mix_in = np.empty((0, tmp_mix.shape[1], tmp_mix.shape[2], tmp_mix.shape[3]), dtype=np.float32)\n",
        "                s_out = np.empty((0, tmp_s.shape[1], tmp_s.shape[2], tmp_s.shape[3]), dtype=np.float32)\n",
        "                a_out = np.empty((0, tmp_a.shape[1], tmp_a.shape[2], tmp_a.shape[3]), dtype=np.float32)\n",
        "                t_out = np.empty((0, tmp_t.shape[1], tmp_t.shape[2], tmp_t.shape[3]), dtype=np.float32)\n",
        "                b_out = np.empty((0, tmp_b.shape[1], tmp_b.shape[2], tmp_b.shape[3]), dtype=np.float32)\n",
        "\n",
        "            mix_in = np.append(mix_in, tmp_mix, axis=0)\n",
        "            s_out = np.append(s_out, tmp_s, axis=0)\n",
        "            a_out = np.append(a_out, tmp_a, axis=0)\n",
        "            t_out = np.append(t_out, tmp_t, axis=0)\n",
        "            b_out = np.append(b_out, tmp_b, axis=0)\n",
        "\n",
        "        mix_in = tf.convert_to_tensor(mix_in, dtype=tf.float32)\n",
        "        s_out = tf.convert_to_tensor(s_out, dtype=tf.float32)\n",
        "        a_out = tf.convert_to_tensor(a_out, dtype=tf.float32)\n",
        "        t_out = tf.convert_to_tensor(t_out, dtype=tf.float32)\n",
        "        b_out = tf.convert_to_tensor(b_out, dtype=tf.float32)\n",
        "\n",
        "        if(self.debug):\n",
        "            time_end = time.time()\n",
        "            time_interval = time_end - time_init\n",
        "            print(time_interval)\n",
        "\n",
        "        return mix_in, (s_out, a_out, t_out, b_out)\n",
        "\n",
        "    def get_splits_per_file(self):\n",
        "        \n",
        "        return self.splits_per_file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQwK19nOu6No"
      },
      "outputs": [],
      "source": [
        "seq = SSCS_Sequence(sscs_get_split()[30:60])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwJsJSObu6No",
        "outputId": "ef0cc26c-e247-42c8-9690-bb5291c43b08"
      },
      "outputs": [],
      "source": [
        "seq.__len__()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        },
        "id": "ucShUHwKMeSR",
        "outputId": "74128fab-83b3-49fd-e783-72b79dbce81d"
      },
      "outputs": [],
      "source": [
        "mix_test, satb_test = seq.__getitem__(2)\n",
        "sscs_plot(mix_test.numpy()[23])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoVmVETou6Np"
      },
      "source": [
        "## 6 - Training VoasCNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwxfnsTTu6Np"
      },
      "outputs": [],
      "source": [
        "dsSpec = tf.TensorSpec(shape=mix_test.shape, dtype=tf.float32)\n",
        "\n",
        "signature = (dsSpec, (dsSpec, dsSpec, dsSpec, dsSpec))\n",
        "\n",
        "ds = tf.data.Dataset.from_generator(SSCS_Sequence,\n",
        "                                    args = [sscs_get_split()[30:60]],\n",
        "                                    output_signature=signature\n",
        "                                    ).cache().prefetch(tf.data.AUTOTUNE)\n",
        "                                    #).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "#tf.data.DatasetSpec.from_value(ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCm9s_bsu6Np"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "class VoasCrossentropy(tf.keras.losses.Loss):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "    def call(self, y_true, y_pred):\n",
        "        time_init = time.time()\n",
        "        y_pred_sq = tf.squeeze(y_pred)\n",
        "        y_true_sq = tf.squeeze(y_true)     \n",
        "        elements =  -tf.math.multiply_no_nan(x=tf.math.log(y_pred_sq),\n",
        "                                        y=y_true_sq) \\\n",
        "                    -tf.math.multiply_no_nan(x=tf.math.log(1 - y_pred_sq),\n",
        "                                        y=(1 - y_true_sq))\n",
        "        print(time.time()-time_init)\n",
        "        return tf.reduce_mean(tf.reduce_sum(elements, range(tf.rank(y_pred_sq))))\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwxeIFWNu6Nq"
      },
      "outputs": [],
      "source": [
        "voas_cnn = voas_cnn_model()\n",
        "voas_cnn.compile(optimizer=Adam(learning_rate=5e-3),\n",
        "                 loss=BinaryCrossentropy(reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE),\n",
        "                 metrics=[Precision()])\n",
        "#voas_cnn.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "NS_REu2Vu6Nq",
        "outputId": "f471e9e7-38e8-4e0e-948d-ca9bbe2318a2"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "if(os.path.exists(checkpoint_dir)):\n",
        "    voas_cnn.load_weights(checkpoint_dir)\n",
        "\n",
        "save_cb = tf.keras.callbacks.ModelCheckpoint(   filepath=checkpoint_dir,\n",
        "                                                save_weights_only=True,\n",
        "                                                verbose=1\n",
        "                                            )\n",
        "\n",
        "voas_cnn.fit(ds, epochs=3, callbacks=[save_cb])\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "voas_cnn.fit(ds, epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "B2MZ_3ghu6Nq",
        "outputId": "6c700708-f12a-4682-fd4e-b108ebae4493"
      },
      "outputs": [],
      "source": [
        "mix, satb = seq.__getitem__(np.random.randint(0, seq.__len__() - 1))\n",
        "\n",
        "mix = mix.numpy()\n",
        "\n",
        "s = satb[0].numpy()\n",
        "a = satb[1].numpy()\n",
        "t = satb[2].numpy()\n",
        "b = satb[3].numpy()\n",
        "\n",
        "s_pred, a_pred, t_pred, b_pred = voas_cnn.predict(mix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "BsAHWtRlu6Nr",
        "outputId": "00f9291f-686a-46c9-a5a4-7a53d05c4cfe"
      },
      "outputs": [],
      "source": [
        "idx = 10\n",
        "\n",
        "sscs_plot(a[idx])\n",
        "sscs_plot(a_pred[idx])\n",
        "sscs_plot(mix[idx])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
