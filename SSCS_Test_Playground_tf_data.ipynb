{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **SSCS - Test Playground - tf.data**\n",
    "\n",
    "Developed by André Paiva\n",
    "\n",
    "Based on SSCS Dataset created by Helena Cuesta and Emilia Gómez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXECUTE_ON_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import json\n",
    "import math\n",
    "import zipfile\n",
    "import tables\n",
    "import requests\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import Accuracy, Precision\n",
    "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_threads = 6\n",
    "os.environ[\"OMP_NUM_THREADS\"] = str(num_threads)\n",
    "os.environ[\"TF_NUM_INTRAOP_THREADS\"] = str(num_threads)\n",
    "os.environ[\"TF_NUM_INTEROP_THREADS\"] = str(num_threads)\n",
    "\n",
    "tf.config.threading.set_inter_op_parallelism_threads(num_threads)\n",
    "tf.config.threading.set_intra_op_parallelism_threads(num_threads)\n",
    "tf.config.set_soft_device_placement(True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Neural Network Model (VoasCNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_MODEL = True\n",
    "LOAD_MODEL = True\n",
    "SPLIT_SIZE = 128\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def voas_cnn_model():\n",
    "    x_in = Input(shape=(360, SPLIT_SIZE, 1))\n",
    "    \n",
    "    x = BatchNormalization()(x_in)\n",
    "\n",
    "    x = Conv2D(filters=32, kernel_size=(3, 3), padding=\"same\",\n",
    "        activation=\"relu\", name=\"conv1\")(x)\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Conv2D(filters=32, kernel_size=(3, 3), padding=\"same\",\n",
    "        activation=\"relu\", name=\"conv2\")(x)\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Conv2D(filters=16, kernel_size=(70, 3), padding=\"same\",\n",
    "        activation=\"relu\", name=\"conv_harm_1\")(x)\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Conv2D(filters=16, kernel_size=(70, 3), padding=\"same\",\n",
    "        activation=\"relu\", name=\"conv_harm_2\")(x)\n",
    "\n",
    "    ## start four branches now\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    ## branch 1\n",
    "    x1a = Conv2D(filters=16, kernel_size=(3, 3), padding=\"same\",\n",
    "        activation=\"relu\", name=\"conv1a\")(x)\n",
    "\n",
    "    x1a = BatchNormalization()(x1a)\n",
    "\n",
    "    x1b = Conv2D(filters=16, kernel_size=(3, 3), padding=\"same\",\n",
    "        activation=\"relu\", name=\"conv1b\")(x1a)\n",
    "\n",
    "    ## branch 2\n",
    "    x2a = Conv2D(filters=16, kernel_size=(3, 3), padding=\"same\",\n",
    "        activation=\"relu\", name=\"conv2a\")(x)\n",
    "\n",
    "    x2a = BatchNormalization()(x2a)\n",
    "\n",
    "    x2b = Conv2D(filters=16, kernel_size=(3, 3), padding=\"same\",\n",
    "        activation=\"relu\", name=\"conv2b\")(x2a)\n",
    "\n",
    "    ## branch 3\n",
    "\n",
    "    x3a = Conv2D(filters=16, kernel_size=(3, 3), padding=\"same\",\n",
    "        activation=\"relu\", name=\"conv3a\")(x)\n",
    "\n",
    "    x3a = BatchNormalization()(x3a)\n",
    "\n",
    "    x3b = Conv2D(filters=16, kernel_size=(3, 3), padding=\"same\",\n",
    "        activation=\"relu\", name=\"conv3b\")(x3a)\n",
    "\n",
    "    x4a = Conv2D(filters=16, kernel_size=(3, 3), padding=\"same\",\n",
    "        activation=\"relu\", name=\"conv4a\")(x)\n",
    "\n",
    "    x4a = BatchNormalization()(x4a)\n",
    "\n",
    "    x4b = Conv2D(filters=16, kernel_size=(3, 3), padding=\"same\",\n",
    "        activation=\"relu\", name=\"conv4b\"\n",
    "    )(x4a)\n",
    "\n",
    "\n",
    "    y1 = Conv2D(filters=1, kernel_size=1, name='conv_soprano',\n",
    "                padding='same', activation='sigmoid')(x1b)\n",
    "    y1 = tf.squeeze(y1, axis=-1)\n",
    "    y2 = Conv2D(filters=1, kernel_size=1, name='conv_alto',\n",
    "                padding='same', activation='sigmoid')(x2b)\n",
    "    y2 = tf.squeeze(y2, axis=-1)\n",
    "    y3 = Conv2D(filters=1, kernel_size=1, name='conv_tenor',\n",
    "                padding='same', activation='sigmoid')(x3b)\n",
    "    y3 = tf.squeeze(y3, axis=-1)\n",
    "    y4 = Conv2D(filters=1, kernel_size=1, name='conv_bass',\n",
    "                padding='same', activation='sigmoid')(x4b)\n",
    "    y4 = tf.squeeze(y4, axis=-1)\n",
    "\n",
    "    out = [y1, y2, y3, y4]\n",
    "\n",
    "    model = Model(inputs=x_in, outputs=out, name='voasCNN')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Auxiliar functions and Variables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions designed to manipulate the SSCS dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 - File path variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(EXECUTE_ON_COLAB):\n",
    "    dataset_dir = \"/content/Datasets/\"\n",
    "else:\n",
    "    dataset_dir = \"Datasets/\"\n",
    "zipname = dataset_dir + \"SSCS_HDF5.zip\"\n",
    "sscs_dir = dataset_dir + \"SSCS_HDF5/\"\n",
    "\n",
    "songs_dir = sscs_dir + \"sscs/\"\n",
    "splitname = sscs_dir + \"sscs_splits.json\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Download/Extract Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(url, fname):\n",
    "    \n",
    "    resp = requests.get(url, stream=True)\n",
    "    total = int(resp.headers.get('content-length', 0))\n",
    "    downloaded_size = 0\n",
    "    with open(fname, 'wb') as file:\n",
    "        for data in resp.iter_content(chunk_size=max(4096, int(total/10000))):\n",
    "            size = file.write(data)\n",
    "            downloaded_size += size\n",
    "            percent = min(downloaded_size/total, 1.0)\n",
    "            print(f\"\\r{percent:.2%} downloaded\", end='')\n",
    "            \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sscs_download():\n",
    "    \n",
    "    if(not os.path.exists(dataset_dir)):\n",
    "        os.mkdir(dataset_dir)\n",
    "   \n",
    "    if(not os.path.exists(zipname)):\n",
    "        print(\"Downloading SSCS Dataset...\")\n",
    "        url = \"https://github.com/Xornotor/SSCS_HDF5/releases/download/v1.0/SSCS_HDF5.zip\"\n",
    "        download(url, zipname)\n",
    "    else:\n",
    "        print(\"SSCS Dataset found.\")\n",
    "\n",
    "    if(not os.path.exists(sscs_dir)):\n",
    "        print(\"Extracting SSCS Dataset...\")\n",
    "        with zipfile.ZipFile(zipname) as zf:\n",
    "            os.mkdir(sscs_dir)\n",
    "            zf.extractall(path=sscs_dir)\n",
    "    else:\n",
    "        print(\"SSCS Dataset already extracted.\")\n",
    "    \n",
    "\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 - Splits, songnames and songlists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sscs_get_split(split='train'):\n",
    "    \n",
    "    if(split.lower() == 'train' or split.lower() == 'validate' or\n",
    "       split.lower() == 'test'):\n",
    "        return json.load(open(splitname, 'r'))[split.lower()]\n",
    "    else:\n",
    "        raise NameError(\"Split should be 'train', 'validate' or 'test'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sscs_pick_songlist(first=0, amount=5, split='train'):\n",
    "    \n",
    "    songnames = sscs_get_split(split)\n",
    "    return songnames[first:first+amount]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sscs_pick_random_song(split='train'):\n",
    "    \n",
    "    songnames = sscs_get_split(split)\n",
    "    rng = np.random.randint(0, len(songnames))\n",
    "    return songnames[rng]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sscs_pick_multiple_random_songs(amount, split='train'):\n",
    "    \n",
    "    return [sscs_pick_random_song() for i in range(amount)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 - Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sscs_plot(dataframe):\n",
    "\n",
    "    aspect_ratio = (3/8)*dataframe.shape[1]/dataframe.shape[0]\n",
    "    fig, ax = plt.subplots(figsize=(13, 7))\n",
    "    im = ax.imshow(dataframe, interpolation='nearest', aspect=aspect_ratio,\n",
    "        cmap = mpl.colormaps['BuPu'])\n",
    "    ax.invert_yaxis()\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Download and extract dataset SSCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sscs_download()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Dataset Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSCS_Sequence(tf.keras.utils.Sequence):\n",
    "    def __init__(self, filenames):\n",
    "        self.filenames = filenames\n",
    "        self.splits_amount = 0\n",
    "        self.splits_per_file = np.array([], dtype=np.intc)\n",
    "        self.songs_dir = songs_dir\n",
    "        self.split_size = SPLIT_SIZE\n",
    "\n",
    "        for file in self.filenames:\n",
    "            file_access = self.songs_dir + file + \".h5\"\n",
    "            f = h5py.File(file_access, 'r')\n",
    "            file_shape = f['mix/table'].shape[0]\n",
    "            df_batch_items = file_shape//self.split_size\n",
    "            #if(file_shape/self.split_size > df_batch_items): df_batch_items += 1\n",
    "            self.splits_per_file = np.append(self.splits_per_file, int(df_batch_items))\n",
    "            self.splits_amount += df_batch_items\n",
    "            f.close()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.splits_amount\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        splits_past = idx\n",
    "        idx_get = 0\n",
    "        if(idx > 0):\n",
    "            while(splits_past > 0):\n",
    "                splits_past -= self.splits_per_file[idx_get]\n",
    "                if(splits_past > 0):\n",
    "                    idx_get += 1\n",
    "        split_get = self.splits_per_file[idx_get] + splits_past\n",
    "        \n",
    "        if(split_get == self.splits_per_file[idx_get]):\n",
    "            idx_get += 1\n",
    "            split_get = 0\n",
    "\n",
    "        file_access = self.songs_dir + self.filenames[idx_get] + \".h5\"\n",
    "        f = h5py.File(file_access, 'r')\n",
    "        data_min = split_get * self.split_size\n",
    "        data_max = data_min + self.split_size\n",
    "\n",
    "        mix_in = np.transpose(pd.read_hdf(file_access, 'mix')[data_min:data_max]).to_numpy()\n",
    "        s_out = np.transpose(pd.read_hdf(file_access, 'soprano')[data_min:data_max]).to_numpy()\n",
    "        a_out = np.transpose(pd.read_hdf(file_access, 'alto')[data_min:data_max]).to_numpy()\n",
    "        t_out = np.transpose(pd.read_hdf(file_access, 'tenor')[data_min:data_max]).to_numpy()\n",
    "        b_out = np.transpose(pd.read_hdf(file_access, 'bass')[data_min:data_max]).to_numpy()\n",
    "\n",
    "        mix_in = mix_in.reshape((mix_in.shape[0], mix_in.shape[1], 1))\n",
    "        s_out = s_out.reshape((s_out.shape[0], s_out.shape[1], 1))\n",
    "        a_out = a_out.reshape((a_out.shape[0], a_out.shape[1], 1))\n",
    "        t_out = t_out.reshape((t_out.shape[0], t_out.shape[1], 1))\n",
    "        b_out = b_out.reshape((b_out.shape[0], b_out.shape[1], 1))\n",
    "\n",
    "        mix_in = tf.convert_to_tensor(mix_in, dtype=tf.float32)\n",
    "        s_out = tf.convert_to_tensor(s_out, dtype=tf.float32)\n",
    "        a_out = tf.convert_to_tensor(a_out, dtype=tf.float32)\n",
    "        t_out = tf.convert_to_tensor(t_out, dtype=tf.float32)\n",
    "        b_out = tf.convert_to_tensor(b_out, dtype=tf.float32)\n",
    "\n",
    "        f.close()\n",
    "\n",
    "        return mix_in, [s_out, a_out, t_out, b_out]\n",
    "\n",
    "    def get_splits_per_file(self):\n",
    "        return self.splits_per_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = SSCS_Sequence(sscs_get_split()[30:60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq.__len__()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - Plot test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mix, satb = seq.__getitem__(np.random.randint(0, seq.__len__()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sscs_plot(mix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 - Training VoasCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tf.data.Dataset.from_generator(SSCS_Sequence, np.float32, (360, 128), [sscs_get_split()[30:60]])\n",
    "\n",
    "for elem in ds:\n",
    "    print(elem.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoasCrossentropy(tf.keras.losses.Loss):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def call(self, y_true, y_pred):\n",
    "        log_y_pred = tf.math.log(y_pred)\n",
    "        log_y_pred_comp = tf.math.log(1 - y_pred)\n",
    "        aux1 = -tf.math.multiply_no_nan(x=log_y_pred, y=y_true)\n",
    "        aux2 = -tf.math.multiply_no_nan(x=log_y_pred_comp, y=(1 - y_true))\n",
    "        elements = aux1 + aux2\n",
    "        return tf.reduce_mean(tf.reduce_sum(elements, range(tf.rank(y_pred))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voas_cnn = voas_cnn_model()\n",
    "voas_cnn.compile(optimizer=Adam(learning_rate=2e-3),\n",
    "                 loss=VoasCrossentropy(),\n",
    "                 metrics=[Precision()])\n",
    "#voas_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voas_cnn.fit(seq, batch_size=32, epochs=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
