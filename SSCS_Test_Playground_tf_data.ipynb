{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **SSCS - Test Playground - tf.data**\n",
    "\n",
    "Developed by André Paiva\n",
    "\n",
    "Based on SSCS Dataset created by Helena Cuesta and Emilia Gómez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXECUTE_ON_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import json\n",
    "import zipfile\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import Accuracy, Precision\n",
    "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_threads = 6\n",
    "os.environ[\"OMP_NUM_THREADS\"] = str(num_threads)\n",
    "os.environ[\"TF_NUM_INTRAOP_THREADS\"] = str(num_threads)\n",
    "os.environ[\"TF_NUM_INTEROP_THREADS\"] = str(num_threads)\n",
    "\n",
    "tf.config.threading.set_inter_op_parallelism_threads(num_threads)\n",
    "tf.config.threading.set_intra_op_parallelism_threads(num_threads)\n",
    "tf.config.set_soft_device_placement(True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Neural Network Model (VoasCNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_MODEL = True\n",
    "LOAD_MODEL = True\n",
    "SPLIT_SIZE = 128\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def voas_cnn_model():\n",
    "    x_in = Input(shape=(360, SPLIT_SIZE, 1))\n",
    "    \n",
    "    x = BatchNormalization()(x_in)\n",
    "\n",
    "    x = Conv2D(filters=32, kernel_size=(3, 3), padding=\"same\",\n",
    "        activation=\"relu\", name=\"conv1\")(x)\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Conv2D(filters=32, kernel_size=(3, 3), padding=\"same\",\n",
    "        activation=\"relu\", name=\"conv2\")(x)\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Conv2D(filters=16, kernel_size=(70, 3), padding=\"same\",\n",
    "        activation=\"relu\", name=\"conv_harm_1\")(x)\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Conv2D(filters=16, kernel_size=(70, 3), padding=\"same\",\n",
    "        activation=\"relu\", name=\"conv_harm_2\")(x)\n",
    "\n",
    "    ## start four branches now\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    ## branch 1\n",
    "    x1a = Conv2D(filters=16, kernel_size=(3, 3), padding=\"same\",\n",
    "        activation=\"relu\", name=\"conv1a\")(x)\n",
    "\n",
    "    x1a = BatchNormalization()(x1a)\n",
    "\n",
    "    x1b = Conv2D(filters=16, kernel_size=(3, 3), padding=\"same\",\n",
    "        activation=\"relu\", name=\"conv1b\")(x1a)\n",
    "\n",
    "    ## branch 2\n",
    "    x2a = Conv2D(filters=16, kernel_size=(3, 3), padding=\"same\",\n",
    "        activation=\"relu\", name=\"conv2a\")(x)\n",
    "\n",
    "    x2a = BatchNormalization()(x2a)\n",
    "\n",
    "    x2b = Conv2D(filters=16, kernel_size=(3, 3), padding=\"same\",\n",
    "        activation=\"relu\", name=\"conv2b\")(x2a)\n",
    "\n",
    "    ## branch 3\n",
    "\n",
    "    x3a = Conv2D(filters=16, kernel_size=(3, 3), padding=\"same\",\n",
    "        activation=\"relu\", name=\"conv3a\")(x)\n",
    "\n",
    "    x3a = BatchNormalization()(x3a)\n",
    "\n",
    "    x3b = Conv2D(filters=16, kernel_size=(3, 3), padding=\"same\",\n",
    "        activation=\"relu\", name=\"conv3b\")(x3a)\n",
    "\n",
    "    x4a = Conv2D(filters=16, kernel_size=(3, 3), padding=\"same\",\n",
    "        activation=\"relu\", name=\"conv4a\")(x)\n",
    "\n",
    "    x4a = BatchNormalization()(x4a)\n",
    "\n",
    "    x4b = Conv2D(filters=16, kernel_size=(3, 3), padding=\"same\",\n",
    "        activation=\"relu\", name=\"conv4b\"\n",
    "    )(x4a)\n",
    "\n",
    "\n",
    "    y1 = Conv2D(filters=1, kernel_size=1, name='conv_soprano',\n",
    "                padding='same', activation='sigmoid')(x1b)\n",
    "    y1 = tf.squeeze(y1, axis=-1)\n",
    "    y2 = Conv2D(filters=1, kernel_size=1, name='conv_alto',\n",
    "                padding='same', activation='sigmoid')(x2b)\n",
    "    y2 = tf.squeeze(y2, axis=-1)\n",
    "    y3 = Conv2D(filters=1, kernel_size=1, name='conv_tenor',\n",
    "                padding='same', activation='sigmoid')(x3b)\n",
    "    y3 = tf.squeeze(y3, axis=-1)\n",
    "    y4 = Conv2D(filters=1, kernel_size=1, name='conv_bass',\n",
    "                padding='same', activation='sigmoid')(x4b)\n",
    "    y4 = tf.squeeze(y4, axis=-1)\n",
    "\n",
    "    out = [y1, y2, y3, y4]\n",
    "\n",
    "    model = Model(inputs=x_in, outputs=out, name='voasCNN')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Auxiliar functions and Variables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions designed to manipulate the SSCS dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 - File path variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(EXECUTE_ON_COLAB):\n",
    "    dataset_dir = \"/content/Datasets/\"\n",
    "else:\n",
    "    dataset_dir = \"Datasets/\"\n",
    "zipname = dataset_dir + \"SSCS_HDF5.zip\"\n",
    "sscs_dir = dataset_dir + \"SSCS_HDF5/\"\n",
    "\n",
    "songs_dir = sscs_dir + \"sscs/\"\n",
    "splitname = sscs_dir + \"sscs_splits.json\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Download/Extract Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(url, fname):\n",
    "    \n",
    "    resp = requests.get(url, stream=True)\n",
    "    total = int(resp.headers.get('content-length', 0))\n",
    "    downloaded_size = 0\n",
    "    with open(fname, 'wb') as file:\n",
    "        for data in resp.iter_content(chunk_size=max(4096, int(total/10000))):\n",
    "            size = file.write(data)\n",
    "            downloaded_size += size\n",
    "            percent = min(downloaded_size/total, 1.0)\n",
    "            print(f\"\\r{percent:.2%} downloaded\", end='')\n",
    "            \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sscs_download():\n",
    "    \n",
    "    if(not os.path.exists(dataset_dir)):\n",
    "        os.mkdir(dataset_dir)\n",
    "   \n",
    "    if(not os.path.exists(zipname)):\n",
    "        print(\"Downloading SSCS Dataset...\")\n",
    "        url = \"https://github.com/Xornotor/SSCS_HDF5/releases/download/v1.0/SSCS_HDF5.zip\"\n",
    "        download(url, zipname)\n",
    "    else:\n",
    "        print(\"SSCS Dataset found.\")\n",
    "\n",
    "    if(not os.path.exists(sscs_dir)):\n",
    "        print(\"Extracting SSCS Dataset...\")\n",
    "        with zipfile.ZipFile(zipname) as zf:\n",
    "            os.mkdir(sscs_dir)\n",
    "            zf.extractall(path=sscs_dir)\n",
    "    else:\n",
    "        print(\"SSCS Dataset already extracted.\")\n",
    "    \n",
    "\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 - Splits, songnames and songlists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sscs_get_split(split='train'):\n",
    "    \n",
    "    if(split.lower() == 'train' or split.lower() == 'validate' or\n",
    "       split.lower() == 'test'):\n",
    "        return json.load(open(splitname, 'r'))[split.lower()]\n",
    "    else:\n",
    "        raise NameError(\"Split should be 'train', 'validate' or 'test'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sscs_pick_songlist(first=0, amount=5, split='train'):\n",
    "    \n",
    "    songnames = sscs_get_split(split)\n",
    "    return songnames[first:first+amount]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sscs_pick_random_song(split='train'):\n",
    "    \n",
    "    songnames = sscs_get_split(split)\n",
    "    rng = np.random.randint(0, len(songnames))\n",
    "    return songnames[rng]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sscs_pick_multiple_random_songs(amount, split='train'):\n",
    "    \n",
    "    return [sscs_pick_random_song() for i in range(amount)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Download and extract dataset SSCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSCS Dataset found.\n",
      "SSCS Dataset already extracted.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "sscs_download()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Dataset Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSCS_Sequence(tf.keras.utils.Sequence):\n",
    "    def __init__(self, filenames, batch_size=BATCH_SIZE):\n",
    "        self.filenames = filenames\n",
    "        self.batch_size = batch_size\n",
    "        self.batch_length = 0\n",
    "        self.batches_per_file = np.array([], dtype=np.intc)\n",
    "        self.songs_dir = songs_dir\n",
    "        self.split_size = SPLIT_SIZE\n",
    "\n",
    "        for file in self.filenames:\n",
    "            file_access = self.songs_dir + file + \".h5\"\n",
    "            f = h5py.File(file_access, 'r')\n",
    "            file_shape = f['mix/table'].shape[0]\n",
    "            df_batches = file_shape//self.split_size\n",
    "            if(file_shape/self.split_size > df_batches):\n",
    "                df_batches += 1\n",
    "            self.batches_per_file = np.append(self.batches_per_file, int(df_batches))\n",
    "            self.batch_length += df_batches\n",
    "            f.close()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.batch_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        print(\"Not yet implemented!\")\n",
    "        pass\n",
    "\n",
    "    def get_batches_per_file(self):\n",
    "        return self.batches_per_file\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = SSCS_Sequence(sscs_get_split(), BATCH_SIZE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
